Code Analysis
Google Search URL Formatting:

The code constructs the Google search URL using the query entered by the user. However, Google's search engine may not reliably serve results in an easily parseable format using a simple HTTP request. Google may block such requests or return unexpected HTML structures, which could make scraping unreliable.
Parsing Search Results:

The code uses the CSS selector "a[href]" to find all links on the page, and then filters them based on the URL starting with "/url?q=". This approach works under the assumption that the search result URLs will always be formatted in this way, which might not always hold true due to changes in Google's HTML structure.
User-Agent String:

The code sets a user-agent string ("Mozilla/5.0") when making the HTTP request. This is good practice to avoid being blocked by Google's servers, but using a more complete and updated user-agent string might yield better results.
Error Handling:

The code handles IOException but does not consider other potential exceptions, such as network errors, or invalid input. Improving error handling would make the code more robust.
Closing Scanner:

The scanner is properly closed after use, which is good practice to avoid resource leaks.
Output File Handling:

The code writes the scraped data to "scrapeout.txt". While this is functional, there is no check to handle the case where the file might already exist, leading to potential data loss. Consider appending to the file or confirming before overwriting.
Suggested Improvements
More Robust URL Handling:

Instead of simply replacing spaces with "+", consider encoding the query string properly using URLEncoder.encode(query, "UTF-8"). This ensures that special characters in the query are correctly handled.
Improved Error Handling:

Add more detailed error handling to catch and manage different types of exceptions. This will make the program more resilient to various runtime issues.
Consider JSON Parsing:

If Google provides a structured JSON response for certain queries (such as using the Google Custom Search API), consider using that instead of HTML scraping. This would be more reliable.
Handle Existing Files:

Add logic to handle existing output files, either by confirming before overwriting or by appending data.
Enhanced Logging:

Provide more detailed logging or user feedback to help debug issues or understand the programâ€™s progress.